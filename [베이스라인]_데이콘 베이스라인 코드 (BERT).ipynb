{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. 데이터 및 라이브러리 불러오기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 코드: 텐서플로2와 머신러닝으로 시작하는 자연어처리(위키북스)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.29.0\n",
      "  Using cached transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (4.59.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (2021.4.4)\n",
      "Requirement already satisfied: requests in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (2.25.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (20.9)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.29.0) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers==4.29.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers==4.29.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers==4.29.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from requests->transformers==4.29.0) (4.0.0)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.29.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in c:\\users\\82102\\anaconda3\\lib\\site-packages (6.2.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (20.3.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (20.9)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (1.10.0)\n",
      "Requirement already satisfied: toml in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (0.10.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from packaging->pytest) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in c:\\users\\82102\\anaconda3\\lib\\site-packages (6.2.3)\n",
      "Collecting pytest\n",
      "  Downloading pytest-8.0.0-py3-none-any.whl (334 kB)\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (20.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (0.4.4)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\82102\\anaconda3\\lib\\site-packages (from pytest) (1.1.1)\n",
      "Collecting pluggy<2.0,>=1.3.0\n",
      "  Downloading pluggy-1.4.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from packaging->pytest) (2.4.7)\n",
      "Installing collected packages: tomli, pluggy, exceptiongroup, pytest\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 0.13.1\n",
      "    Uninstalling pluggy-0.13.1:\n",
      "      Successfully uninstalled pluggy-0.13.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 6.2.3\n",
      "    Uninstalling pytest-6.2.3:\n",
      "      Successfully uninstalled pytest-6.2.3\n",
      "Successfully installed exceptiongroup-1.2.0 pluggy-1.4.0 pytest-8.0.0 tomli-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\anaconda3\\lib\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\82102\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\anaconda3\\lib\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score,f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "sample_submission=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape:(174304, 13)\n",
      "test.shape:(43576, 12)\n",
      "train label 개수: 46\n"
     ]
    }
   ],
   "source": [
    "print(f'train.shape:{train.shape}')\n",
    "print(f'test.shape:{test.shape}')\n",
    "print(f'train label 개수: {train.label.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. 데이터 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이번 베이스라인에서는 과제명 뿐만 아니라 요약문_연구내용도 모델에 학습시켜보겠습니다.\n",
    "train=train[['과제명', '요약문_연구내용','label']]\n",
    "test=test[['과제명', '요약문_연구내용']]\n",
    "train['요약문_연구내용'].fillna('NAN', inplace=True)\n",
    "test['요약문_연구내용'].fillna('NAN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['data']=train['과제명']+train['요약문_연구내용']\n",
    "test['data']=test['과제명']+test['요약문_연구내용']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174304, 4)\n",
      "(43576, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과제명</th>\n",
       "      <th>요약문_연구내용</th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>유전정보를 활용한 새로운 해충 분류군 동정기술 개발</td>\n",
       "      <td>(가) 외래 및 돌발해충의 발생조사 및 종 동정\\n\\n\\n    ○ 대상해충 : 최...</td>\n",
       "      <td>24</td>\n",
       "      <td>유전정보를 활용한 새로운 해충 분류군 동정기술 개발(가) 외래 및 돌발해충의 발생조...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...</td>\n",
       "      <td>1차년도\\n1) Microarray를 통한 선천적 TRAIL 내성 표적 후보 유전자...</td>\n",
       "      <td>0</td>\n",
       "      <td>대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 과제명  \\\n",
       "0                       유전정보를 활용한 새로운 해충 분류군 동정기술 개발   \n",
       "1  대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...   \n",
       "\n",
       "                                            요약문_연구내용  label  \\\n",
       "0  (가) 외래 및 돌발해충의 발생조사 및 종 동정\\n\\n\\n    ○ 대상해충 : 최...     24   \n",
       "1  1차년도\\n1) Microarray를 통한 선천적 TRAIL 내성 표적 후보 유전자...      0   \n",
       "\n",
       "                                                data  \n",
       "0  유전정보를 활용한 새로운 해충 분류군 동정기술 개발(가) 외래 및 돌발해충의 발생조...  \n",
       "1  대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과제명</th>\n",
       "      <th>요약문_연구내용</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...</td>\n",
       "      <td>○ 1차년도\\n\\n    . 개발 탐촉 시스템의 성능 평가 위한 표준 시편 제작 시...</td>\n",
       "      <td>R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>다입자계를 묘사하는 편미분방정식에 대한 연구</td>\n",
       "      <td>연구과제1. 무한입자계의 동역학 / 작용소(operator) 방정식에 대한 연구\\n...</td>\n",
       "      <td>다입자계를 묘사하는 편미분방정식에 대한 연구연구과제1. 무한입자계의 동역학 / 작용...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 과제명  \\\n",
       "0  R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...   \n",
       "1                           다입자계를 묘사하는 편미분방정식에 대한 연구   \n",
       "\n",
       "                                            요약문_연구내용  \\\n",
       "0  ○ 1차년도\\n\\n    . 개발 탐촉 시스템의 성능 평가 위한 표준 시편 제작 시...   \n",
       "1  연구과제1. 무한입자계의 동역학 / 작용소(operator) 방정식에 대한 연구\\n...   \n",
       "\n",
       "                                                data  \n",
       "0  R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...  \n",
       "1  다입자계를 묘사하는 편미분방정식에 대한 연구연구과제1. 무한입자계의 동역학 / 작용...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. 모델링**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer 이해 : https://hyen4110.tistory.com/89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd0cdaf785349e68906642d51a12f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\82102\\주피터노트북\\2-2 겨울방학\\[데이콘 연습] - 자연어 기반 기후기술분류\\bert_ckpt\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24db0edb702c4e9aa86d82df98696639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at bert_ckpt\\models--bert-base-multilingual-cased\\snapshots\\fdfce55e83dbed325647a63e7e1f5de19f0382ba\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at bert_ckpt\\models--bert-base-multilingual-cased\\snapshots\\fdfce55e83dbed325647a63e7e1f5de19f0382ba\\tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e243f32e09415f8b48e1934fe786ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at bert_ckpt\\models--bert-base-multilingual-cased\\snapshots\\fdfce55e83dbed325647a63e7e1f5de19f0382ba\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.29.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',  cache_dir='bert_ckpt', do_lower_case=False)\n",
    "\n",
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    \n",
    "    encoded_dict=tokenizer.encode_plus(\n",
    "    text = sent, \n",
    "    add_special_tokens=True, \n",
    "    max_length=MAX_LEN, \n",
    "    pad_to_max_length=True, \n",
    "    return_attention_mask=True,\n",
    "    truncation = True)\n",
    "    \n",
    "    input_id=encoded_dict['input_ids']\n",
    "    attention_mask=encoded_dict['attention_mask']\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\n",
    "    return sent_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids =[]\n",
    "attention_masks =[]\n",
    "token_type_ids =[]\n",
    "train_data_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for train_sent, train_label in zip(train['data'], train['label']):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(train_sent), MAX_LEN=MAX_LEN)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        #########################################\n",
    "        train_data_labels.append(train_label)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids=np.array(input_ids, dtype=int)\n",
    "train_attention_masks=np.array(attention_masks, dtype=int)\n",
    "train_token_type_ids=np.array(token_type_ids, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174304, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  101,  9625, 16617, ...,     0,     0,     0],\n",
       "       [  101,  9069, 13890, ..., 74986, 17138,   102],\n",
       "       [  101,  9379, 68055, ..., 36210,  9891,   102],\n",
       "       ...,\n",
       "       [  101,  9486, 29364, ..., 13441,  9087,   102],\n",
       "       [  101,  9819,  9625, ..., 91785, 21789,   102],\n",
       "       [  101,  8903, 87503, ..., 15001, 63218,   102]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(train_input_ids))\n",
    "train_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174304, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(train_attention_masks))\n",
    "train_attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174304, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(train_token_type_ids))\n",
    "train_token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs=(train_input_ids, train_attention_masks, train_token_type_ids)\n",
    "train_labels=np.asarray(train_data_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   101   9069  13890 119115  10459   8996  17138   9934  14801   9640\n",
      "  13764   9323 118654   9316   9321 119187   9576 119281   9625  16617\n",
      "  13764   9706  12092   8908  70122  10530  42300  91785   9730  10954\n",
      "  12092   9233   9879  11102   9428  38631  14801   8996  17138   9934\n",
      "  14801  10003  30005   9625  16617  13764   9428  61844   9069  13890\n",
      " 119115   9995  13764  20626  33077  10622   9638  61689  10003  30005\n",
      "   9625  16617  42984   9323  30842  11882   9323  30842   9543  14871\n",
      "   9367  40958  10003  30005   9625  16617  13764   9323  30842   9678\n",
      "  58931  10622   9638  65219   9934  14801   8843  74986  17138   9316\n",
      "   8996  17138   9672  12965   8932  16617   8922  16758   9934  14801\n",
      "   9625  16617  13764   9246  89108   8908  70122   9316   9095  29364\n",
      "  39420 118791  10622   9879  11102  10003  30005   9625  16617  13764\n",
      "   9934  14801   8843  74986  17138   8868 119230   9428  38631  14801\n",
      "   8996  17138   9576 119281  12030  13764   9323 118654   9730  10954\n",
      "  12092   9233   9879  11102  10003  38631  14801   8996  17138   9934\n",
      "  14801  10003  30005   9625  16617  13764   9428  61844   9069  13890\n",
      " 119115   9995  13764  20626  33077  10622   9638  61689  10003  30005\n",
      "   9625  16617  42984   9323  30842  11882   9323  30842   9543  14871\n",
      "   9367  40958  10003  30005   9625  16617  13764   9323  30842   9678\n",
      "  58931  10622   9638  65219   9934  14801   8843  74986  17138    102]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[CLS] 대장암의 내성 표적 인자 발굴 및 반응 예측 유전자 지도 구축에 관한 연구 차년도 를 통한 선천적 내성 표적 후보 유전자 선별 대장암 환자조직을 이용하여 후보 유전자의 발현과 발현 양상 분석 후보 유전자 발현 조절을 이용한 표적 가능성 및 내성 제어 기전 규명 표적 유전자 마우스 구축 및 동물모델을 통한 후보 유전자 표적 가능성 검증 선천적 내성 예측인자 발굴 차년도 를 통한 후천적 내성 표적 후보 유전자 선별 대장암 환자조직을 이용하여 후보 유전자의 발현과 발현 양상 분석 후보 유전자 발현 조절을 이용한 표적 가능성 [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(train_input_ids[1])\n",
    "print(train_attention_masks[1])\n",
    "print(train_token_type_ids[1])\n",
    "print(tokenizer.decode(train_input_ids[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertClassifier, self).__init__()\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
    "                                                name=\"classifier\")\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            # outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "            outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            pooled_output = outputs[1] \n",
    "            pooled_output = self.dropout(pooled_output, training=training)\n",
    "            logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at bert_ckpt\\models--bert-base-multilingual-cased\\snapshots\\fdfce55e83dbed325647a63e7e1f5de19f0382ba\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.29.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at bert_ckpt\\models--bert-base-multilingual-cased\\snapshots\\fdfce55e83dbed325647a63e7e1f5de19f0382ba\\model.safetensors\n",
      "Loaded 177,853,440 parameters in the TF 2.0 model.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 준비하기\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tf2_bert_classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=5)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf2_bert_classifier -- Folder create complete \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x00000234C1811AF0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function input_processing at 0x00000234C1811AF0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "# 학습과 eval 시작\n",
    "history = cls_model.fit(train_inputs, train_labels, epochs=30, batch_size=32,\n",
    "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e4d0b8a8cb46178b85af44d51d4985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b964e9e214f449ebee785da807a1ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf2_bert_classifier -- Folder already exists \n",
      "\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1315aa1048>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f13140d6048> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1315aa1048>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f13140d6048> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "4358/4358 [==============================] - ETA: 0s - loss: 0.7767 - accuracy: 0.8298WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "4358/4358 [==============================] - 1374s 312ms/step - loss: 0.7766 - accuracy: 0.8298 - val_loss: 0.5007 - val_accuracy: 0.8547\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85465, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 2/30\n",
      "4358/4358 [==============================] - 1362s 312ms/step - loss: 0.4176 - accuracy: 0.8752 - val_loss: 0.4047 - val_accuracy: 0.8804\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.85465 to 0.88041, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 3/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.3247 - accuracy: 0.8992 - val_loss: 0.3749 - val_accuracy: 0.8908\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.88041 to 0.89079, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 4/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.2644 - accuracy: 0.9155 - val_loss: 0.3709 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.89079 to 0.89934, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 5/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.2158 - accuracy: 0.9311 - val_loss: 0.3674 - val_accuracy: 0.8949\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89934\n",
      "Epoch 6/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.1799 - accuracy: 0.9421 - val_loss: 0.3722 - val_accuracy: 0.9011\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.89934 to 0.90106, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 7/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.1511 - accuracy: 0.9508 - val_loss: 0.3848 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.90106 to 0.90606, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 8/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.1287 - accuracy: 0.9585 - val_loss: 0.3896 - val_accuracy: 0.9071\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.90606 to 0.90706, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 9/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.1103 - accuracy: 0.9651 - val_loss: 0.3989 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.90706\n",
      "Epoch 10/30\n",
      "4358/4358 [==============================] - 1361s 312ms/step - loss: 0.0992 - accuracy: 0.9685 - val_loss: 0.3865 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.90706 to 0.91237, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 11/30\n",
      "4358/4358 [==============================] - 1362s 312ms/step - loss: 0.0913 - accuracy: 0.9711 - val_loss: 0.4199 - val_accuracy: 0.9108\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.91237\n",
      "Epoch 12/30\n",
      "4358/4358 [==============================] - 1354s 311ms/step - loss: 0.0835 - accuracy: 0.9736 - val_loss: 0.4307 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.91237\n",
      "Epoch 13/30\n",
      "4358/4358 [==============================] - 1357s 311ms/step - loss: 0.0784 - accuracy: 0.9744 - val_loss: 0.4304 - val_accuracy: 0.9116\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.91237\n",
      "Epoch 14/30\n",
      "4358/4358 [==============================] - 1349s 310ms/step - loss: 0.0744 - accuracy: 0.9758 - val_loss: 0.4371 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.91237 to 0.91561, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 15/30\n",
      "4358/4358 [==============================] - 1365s 313ms/step - loss: 0.0705 - accuracy: 0.9779 - val_loss: 0.4498 - val_accuracy: 0.9102\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.91561\n",
      "Epoch 16/30\n",
      "4358/4358 [==============================] - 1364s 313ms/step - loss: 0.0657 - accuracy: 0.9798 - val_loss: 0.4168 - val_accuracy: 0.9139\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.91561\n",
      "Epoch 17/30\n",
      "4358/4358 [==============================] - 1351s 310ms/step - loss: 0.0635 - accuracy: 0.9799 - val_loss: 0.4741 - val_accuracy: 0.9065\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.91561\n",
      "Epoch 18/30\n",
      "4358/4358 [==============================] - 1352s 310ms/step - loss: 0.0603 - accuracy: 0.9808 - val_loss: 0.4931 - val_accuracy: 0.9168\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.91561 to 0.91678, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 19/30\n",
      "4358/4358 [==============================] - 1347s 309ms/step - loss: 0.0576 - accuracy: 0.9821 - val_loss: 0.4830 - val_accuracy: 0.9160\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.91678\n",
      "Epoch 20/30\n",
      "4358/4358 [==============================] - 1347s 309ms/step - loss: 0.0571 - accuracy: 0.9815 - val_loss: 0.4743 - val_accuracy: 0.9131\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.91678\n",
      "Epoch 21/30\n",
      "4358/4358 [==============================] - 1346s 309ms/step - loss: 0.0509 - accuracy: 0.9838 - val_loss: 0.4727 - val_accuracy: 0.9134\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.91678\n",
      "Epoch 22/30\n",
      "4358/4358 [==============================] - 1347s 309ms/step - loss: 0.0516 - accuracy: 0.9831 - val_loss: 0.4968 - val_accuracy: 0.9162\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.91678\n",
      "Epoch 23/30\n",
      "4358/4358 [==============================] - 1347s 309ms/step - loss: 0.0503 - accuracy: 0.9844 - val_loss: 0.4817 - val_accuracy: 0.9133\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.91678\n"
     ]
    }
   ],
   "source": [
    "class TFBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
    "                                                name=\"classifier\")\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        \n",
    "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1] \n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=46)\n",
    "\n",
    "# 학습 준비하기\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model_name = \"tf2_bert_classifier\"\n",
    "\n",
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=5)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
    "\n",
    "checkpoint_path = os.path.join(model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# 학습과 eval 시작\n",
    "history = cls_model.fit(train_inputs, train_labels, epochs=30, batch_size=32,\n",
    "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids =[]\n",
    "attention_masks =[]\n",
    "token_type_ids =[]\n",
    "train_data_labels = []\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\n",
    "    return sent_clean\n",
    "\n",
    "for test_sent in test['data']:\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(test_sent), MAX_LEN=40)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        #########################################\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(test_sent)\n",
    "        pass\n",
    "    \n",
    "test_input_ids=np.array(input_ids, dtype=int)\n",
    "test_attention_masks=np.array(attention_masks, dtype=int)\n",
    "test_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "###########################################################\n",
    "test_inputs=(test_input_ids, test_attention_masks, test_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "results = cls_model.predict(test_inputs)\n",
    "results=tf.argmax(results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['label']=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174304</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>174307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43571</th>\n",
       "      <td>217875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43572</th>\n",
       "      <td>217876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43573</th>\n",
       "      <td>217877</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43574</th>\n",
       "      <td>217878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43575</th>\n",
       "      <td>217879</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43576 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  label\n",
       "0      174304     18\n",
       "1      174305      0\n",
       "2      174306      0\n",
       "3      174307      0\n",
       "4      174308      0\n",
       "...       ...    ...\n",
       "43571  217875      0\n",
       "43572  217876      0\n",
       "43573  217877      2\n",
       "43574  217878      0\n",
       "43575  217879     19\n",
       "\n",
       "[43576 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('bert_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
